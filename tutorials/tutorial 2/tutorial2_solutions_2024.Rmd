---
title: "Tutorial 2 Solutions - Thompson Sampling and UCB"
author: "Floris Holstege, Bernhard van der Sluis"
date: "January 2024"
output:
  pdf_document:
    fig_caption: yes
header-includes: 
  \usepackage{float} 
  \usepackage{booktabs} % To thicken table lines
---

```{r setup, include=FALSE}

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )
```

In this problem set, we will be covering Thompson Sampling and Upper Confidence Bound (UCB) methods to solve multi-armed bandit problems. 

# Dataset

Today, we will work with two datasets. For the part about Thompson sampling, we work with a dataset from ZOZO. ZOZO is the largest Japanese fashion e-commerce company. The company uses multi-armed bandit algorithms to recommend fashion items to users in a large-scale fashion e-commerce platform. For the part about UCB, we work with the Yahoo dataset from the first tutorial.

We will first work with a version of the dataset which contains 10 fashion items. Each row contains an fashion item randomly shown to a user, and if the user clicked on said fashion item. It contains +300k observations. The fashion items were randomly shown to users, which makes this dataset suitable for evaluating our TS algorithm. 


```{r}


# this version contains 10 arms (ZOZO)
dfZozo_10 <- read.csv('zozo_noContext_10items.csv')%>%
  select(item_id, click)

# reads in full csv of Yahoo dataset
dfYahoo <- read.csv('../../data/clean/yahoo_data/yahoo_day1_10arms.csv')[,-c(1,2)]

# selects the two relevant columns from Yahoo dataset; arm shown to user and reward observed
dfYahoo_for_sim <- dfYahoo %>% select(arm, reward)
dfYahoo_for_sim$index <- 1:nrow(dfYahoo_for_sim)
```

Below we will first introduce the notation and code for Thompson Sampling. The notation and code for UCB follows as second.

# Thompson Sampling: notation
When working with the $\epsilon$-greedy algorithm, we estimate the expected reward using the average reward up until that point. With Thompson sampling, we take a different approach; instead of expected reward, we try to find out the distribution of the reward. This is done in three steps:
\begin{enumerate}
  \item Let $Q_t(a)$ follow a beta distribution: $f(x: \alpha_t, \beta_t) = \frac{\Gamma(\alpha_t + \beta_t)}{\Gamma(\alpha_t) \cdot \Gamma(\beta_t)}x ^{\alpha_t - 1}(1-x)^{\beta_t - 1}$. At $t=0$, every arm follows the same beta distribution with $\alpha_0=1, \beta_0=1$. 
  \item At each $t$, the algorithm samples $n_{\mathrm{sample}}$ observations from each of the distributions. The arm which has the highest average reward according to the sample is the chosen arm. 
  \item We then observe the reward $r_t$, and update the parameters of the distribution accordingly. 
  \begin{itemize}
    \item $\alpha_t = \alpha_{t-1} + r_t$
    \item $\beta_t = \beta_{t-1} + 1 - r_t$
  \end{itemize}
\end{enumerate}

\textbf{Task 1: select from the dataframe below an arm according to the Thompson Sampling algorithm}. Per arm, the alpha and beta parameters have been given. Use $n_{\mathrm{sample}}=100$.

```{r}

# arm index
arm <- 1:10

# alpha per arm
alpha <- c(10,2,9,3,1,8,7,4,6,5)

# beta per arm
beta <- c(1,8,3, 2,4,7,6,5,9,10)

# dataframe with alpha, beta per arm
df_alpha_beta <- data.frame(arm=arm,alpha = alpha, beta = beta)

# number of samples to draw
n_sample=100

# create dataframe with sampled values
df_sampled <- mapply(rbeta, n_sample, df_alpha_beta$alpha, df_alpha_beta$beta) 

# average per sample
avg_from_sampled <- apply(df_sampled, 2, mean)

# get the arm
chosen_arm <- which.max(avg_from_sampled)

chosen_arm

```

# Thompson Sampling: code

We will be simulating the performance of the Thompson Sampling algorithm on the Zozo data.\\

To do so easily, we are using the _contextual_ package in R. The documentation for this package can be found \href{ https://nth-iteration-labs.github.io/contextual/}{here}. This package allows for easy simulation and implementation of various multi-armed bandit algorithms. For each simulation, one needs to define a (1) bandit, which draws the rewards per arm, (2) an policy/algorithm for pulling arms of the bandit, and (3) an simulator which simulates the performance of the policy/algorithm given the bandit. 

Below, I go over an example of how one can use the package. Let's start by simply applying the Thompson Sampling algorithm to the ZOZO data. We (1) define a bandit, (2) a policy/algorithm and (3) and agent. 


```{r, results='hide', message=FALSE, warning=FALSE}



## OfflineReplayEvaluatorBandit: simulates a bandit based on provided data
#
# Arguments:
#
#   data: dataframe that contains variables with (1) the reward and (2) the arms
#
#   formula: should consist of variable names in the dataframe. General structure is: 
#       reward variable name ~ arm variable name
#
#   randomize: whether or not the bandit should receive the data in random order,
#              or as ordered in the dataframe.
#

# in our case, create a bandit for the data with 20 arms, 
#   formula = click ~ item_id, 
#   no randomization 
bandit_Zozo_10 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,data = dfZozo_10, randomize = FALSE)


# lets generate a 10 simulations, each of size 100.000,
size_sim=100000
n_sim=10

# here we define the Thompson Sampling policy object
TS          <- ThompsonSamplingPolicy$new()


# the contextual package works with 'agent' objects - which consist of a policy and a bandit
agent_TS_zozo_10 <-  Agent$new(TS, # add policy
                               bandit_Zozo_10) # add bandit


## simulator: simulates a bandit + policy based on the provided data and parameters
#
# Arguments:
#
#   agent: the agent object, previously defined
#
#   horizon: how many observations from dataset used in the simulation
#
#   do_parallel: if True, runs simulation in parallel
#
#   simulations: how many simulations?
#

simulator          <- Simulator$new(agent_TS_zozo_10, # set our agent
                                      horizon= size_sim, # set the sizeof each simulation
                                      do_parallel = TRUE, # run in parallel for speed
                                      simulations = n_sim, # simulate it n_sim times
                                  )
  
  
# run the simulator object  
history_TS_zozo_10            <- simulator$run()

# gather results
df_TS_zozo_10 <- history_TS_zozo_10$data %>%
  select(t, sim, choice, reward, agent) 



```

Now that we have gathered the results, let's dive a little bit deeper into each component. First, the bandit object, _OfflineReplayEvaluatorBandit_. The idea behind this function is that it 'replays' the results of an algorithm/policy based on random data. Once an algorithm picks an arm, for instance arm 1, it takes the first observation of arm 1 in the dataset. If the algorithm again picks arm 1, it again samples the next observation from arm 1, etc. 


Let us calculate per simulation (1-10), the maximum number of observations. 

```{r}
df_TS_zozo_10_max_t <- df_TS_zozo_10%>%
  group_by(sim) %>% # group by per agent
  summarize(max_t = max(t)) # get max t

df_TS_zozo_10_max_t
```

\textbf{Task 2: answer below: why is the maximum number of observations different for each simulation?}

\textbf{Answer}: when we use offline replay evaluation, there is only a limited number of observations per arm (if we do not sample with replacement). So if one simulation picks a certain arm more often, at some point there are no more observations to sample. Different simulations reach this point at different moments. 


In the dataframe _df_TS_zozo_ we have gathered the results of the TS policy. This dataframe contains the following columns: 
\begin{itemize}
\item \textbf{$t$}: the step at which a choice was made
\item \textbf{sim}: the simulation for which we observe results
\item \textbf{choice}: the choice made by the algorithm
\item \textbf{reward}: the reward observed by the algorithm
\item \textbf{agent}: column containing the name of the agent 
\end{itemize}

\textbf{Task 3: using this dataframe, make a plot of the average cumulative rewards for all simulations, together with the 95\% confidence interval}. 



```{r}

# Max of observations. Depends on the number of observations per simulation
max_obs=9000
df_history_agg <- df_TS_zozo_10 %>%
  group_by( sim)%>% # group by simulation
  mutate(cumulative_reward = cumsum(reward))%>% # calculate, per sim, cumulative reward over time
  group_by( t) %>% # group by timestep 
  summarise(avg_cumulative_reward = mean(cumulative_reward), # average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm=TRUE)/sqrt(n_sim)) %>% # SE + Confidence interval
  mutate(cumulative_reward_lower_CI =avg_cumulative_reward - 1.96*se_cumulative_reward,
         cumulative_reward_upper_CI =avg_cumulative_reward + 1.96*se_cumulative_reward)%>%
  filter(t <=max_obs)


# define the legend of the plot
legend <- c("Avg." = "orange", "95% CI" = "gray") # set legend

# create ggplot object
ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward))+ 
  geom_line(size=1.5,aes(color="Avg."))+ # add line 
  geom_ribbon(aes(ymin=ifelse(cumulative_reward_lower_CI<0, 0,cumulative_reward_lower_CI),
                  # add confidence interval
                  ymax=cumulative_reward_upper_CI,
                  color = "95% CI"
                 ), # 
              alpha=0.1)+
  labs(x = 'Time', y='Cumulative Reward', color='Metric')+ # add titles
  scale_color_manual(values=legend)+ # add legend
  theme_bw()+ # set the theme
  theme(text = element_text(size=16)) # enlarge text



```

As stated at the start, we only have 10 arms for the ZOZO data. However, there are also versions of the data with 20 and 40 arms. These are loaded for you below. 

\textbf{Task 4: repeat the steps in the tutorial above, but now for the data 40 arms. Make a plot that compares the average cumulative reward (with 95\% confidence interval) for 10 and 40 arms. For which version does Thompson Sampling perform better?}. 


```{r}

# this version contains 20, 40 arms
dfZozo_40 <- read.csv('../../data/clean/zozo_data/zozo_noContext_20items.csv')%>% select(item_id, click)


```


```{r, results='hide', message=FALSE, warning=FALSE}

# define the bandit for zozo data with 10 arms
bandit_Zozo_40 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                   data = dfZozo_40, 
                                                   randomize = FALSE)


# define the size of the simulation, as well as the number of simulations
size_sim=100000
n_sim=10


# define the agent for zozo data with 20, 40 arms
agent_TS_zozo_40 <-  Agent$new(TS, bandit_Zozo_40)


# simulate the results for 10, 20, 40 arms
simulator          <- Simulator$new(list(agent_TS_zozo_10,
                                         agent_TS_zozo_40),
                                      horizon= size_sim, 
                                      do_parallel = TRUE, 
                                      simulations = n_sim, 
                                      progress_file = FALSE)
  

# gather the data in data.frame
history            <- simulator$run()
df_TS_zozo <- history$data %>%
  select(t, sim, choice, reward, agent) 
```


```{r}

# set max observations to create fair comparison across simulations
max_obs=4000


# set the agent variable to number of 
df_TS_zozo$agent <- recode(df_TS_zozo$agent, 'ThompsonSampling' = '10', 'ThompsonSampling.2'='40')

df_history_agg <- df_TS_zozo %>%
  group_by(agent, sim)%>% # group by agent, simulation
  mutate(cumulative_reward = cumsum(reward))%>% # calculate cumulative reward
  group_by(agent, t) %>% # group by agent, timestep t
  summarise(avg_cumulative_reward = mean(cumulative_reward), # calculate average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm=TRUE)/sqrt(n_sim)) %>% # calculate SE + Confidence interval
  mutate(cumulative_reward_lower_CI =avg_cumulative_reward - 1.96*se_cumulative_reward,
         cumulative_reward_upper_CI =avg_cumulative_reward + 1.96*se_cumulative_reward)%>%
  filter(t <=max_obs)


# create ggplot object
ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward, color =agent))+
  geom_line(size=1.5)+ # create line
  geom_ribbon(aes(ymin=ifelse(cumulative_reward_lower_CI<0, 0,cumulative_reward_lower_CI) , # create confidence interval
                  ymax=cumulative_reward_upper_CI,
                  fill = agent,

  ),
  alpha=0.1)+
  labs(x = 'Time', y='Cumulative Reward', color ='number of arms', fill='number of arms')+
  theme_bw()+
  theme(text = element_text(size=16))

```

\newpage
# UCB: notation

As you saw in the lecture, UCB methods decide on the arm to pick using an 'optimistic' estimate of the reward - e.g. which arm has the most potential to yield a high reward? This is done by picking the arm that has the highest combination of (1) the estimated reward, and (2) an bonus that rewards the arm for being underexplored, denoted as $U_t(a)$. For the UCB algorithm, this becomes

\begin{align}
  a_t =  \mathrm{argmax}_{a \in \mathcal{A}} Q_t(a) + c \cdot U_t(a).\\
\end{align}

In the general case of the UCB algorithm,   $U_t(a) = \sqrt{\frac{ log(t)}{N_t(a)}}$, where $N_t(a)$ is the number of times arm $a$ has been pulled so far. $N_t(a)$  grows, the exploration reward becomes smaller. The intuition is here is that as we gather more info about the arm by pulling it, we become more certain about its average reward. The parameter $c$ determines the degree of exploitation vs. exploration - if $c$ is high, we are more likely to explore an arm with potential. 

\textbf{Task 5: select the arm according to the UCB algorithm based on the data below }. Here, the code already provides a dataframe with per arm (1) the average reward, (2) the number of pulls thus far, and (3) an given value for $t$. Use this dataframe to determine which arm should be pulled if $c=0.1$. The selected arm should be 8. 

```{r}
# set the seed
set.seed(0)

# get per item, the average reward and the number of items observed
dfYahoo_summary <- dfYahoo %>%
  group_by(arm) %>%
  summarise(avg_reward = mean(reward),
            n_pulls= n()) 

# the t in this case is simply the total of observations
t <- sum(dfYahoo_summary$n_pulls)
c= 0.1

# get UCB reward 
ucb_reward <- dfYahoo_summary$avg_reward + sqrt(log(t)/dfYahoo_summary$n_pulls)
ucb_ichosen_item = which.max(ucb_reward)
ucb_ichosen_item
```


# UCB: code


We will be simulating the performance of the UCB algorithm on the Yahoo data, again with the contextual package.


```{r, results='hide', message=FALSE, warning=FALSE}

# set the seed
set.seed(0)

## OfflineReplayEvaluatorBandit: simulates a bandit based on provided data
#
# Arguments:
#
#   data: dataframe that contains variables with (1) the reward and (2) the arms
#
#   formula: should consist of variable names in the dataframe. General structure is: 
#       reward variable name ~ arm variable name
#
#   randomize: whether or not the bandit should receive the data in random order,
#              or as ordered in the dataframe.
#

# in our case, create a bandit for the data with 20 arms, 
#   formula = reward ~ arm, 
#   no randomization 
bandit_yahoo <- OfflineReplayEvaluatorBandit$new(formula = reward ~ arm,
                                              data = dfYahoo, 
                                              randomize = FALSE,
                                              )


# lets generate a 10 simulations, each of size 100.000,
size_sim=100000
n_sim=10

# here we define the UCB policy object
# for some reason, the contextual package uses the 'UCB2Policy' tag for standard UCB algorithm
# in the package, 'alpha' is what we refer to as c
UCB_01          <- UCB2Policy$new(alpha=0.1)

# the contextual package works with 'agent' objects - which consist of a policy and a bandit
UCB_01_agent <- Agent$new(UCB_01, # add our UCB1 policy
                        bandit_yahoo) # add our bandit

## simulator: simulates a bandit + policy based on the provided data and parameters
#
# Arguments:
#
#   agent: the agent object, previously defined
#
#   horizon: how many observations from dataset used in the simulation
#
#   do_parallel: if True, runs simulation in parallel
#
#   simulations: how many simulations?
#

simulator          <- Simulator$new(UCB_01_agent, # set our agent
                                      horizon= size_sim, # set the sizeof each simulation
                                      do_parallel = TRUE, # run in parallel for speed
                                      simulations = n_sim, # simulate it n_sim times
                                  )
  
  
# run the simulator object  
history_UCB_01            <- simulator$run()

# gather results
df_Yahoo_UCB_01 <- history_UCB_01$data %>%
  select(t, sim, choice, reward, agent) 




```



In the dataframe _df_Yahoo_UCB_01_ we have gathered the results of the UCB policy for $c=0.1$. 

\textbf{Task 6: repeat the steps of this tutorial, but now for $c=0.5$. Make a plot that compares the UCB policy for $c=0.1$, $c=0.5$. Compare the policies based on average cumulative reward. Can you conclude which one performs better, and if so why?}.Use a maximum of $t=10.000$

\textbf{Answer}: Using the UCB policy with $c=0.5$ scores on average a lower cumulative reward over 10.000 observations than the UCB policy with $c=0.1$. However, this difference is not statistically significant, as the 95\% confidence intervals overlap. \\

The difference in averages is probably due to the fact that at $c=0.5$, the bandit is too focused on exploring arms that potentially yield more reward, rather than sticking to one of the arms. Most likely this is because one of the arms is better. 


```{r,results='hide', message=FALSE, warning=FALSE}

# set the seed
set.seed(0)

# define policy
UCB_05 <- UCB2Policy$new(alpha = 0.5)

# define agent
UCB_05_agent <- Agent$new(UCB_05, bandit_yahoo)


# simulate
simulator          <- Simulator$new(list(UCB_01_agent, UCB_05_agent), # set our agents
                                      horizon= size_sim, # set the sizeof each simulation
                                      do_parallel = TRUE, # run in parallel for speed
                                      simulations = n_sim, # simulate it n_sim times
                                  )
  
  
# run the simulator object  
history_UCB            <- simulator$run()



```


```{r}

# gather results
df_Yahoo_UCB <- history_UCB$data %>%
  select(t, sim, choice, reward, agent) 


# agent replaced with alpha value
df_Yahoo_UCB$agent <- recode(df_Yahoo_UCB$agent, 'UCB2' = '0.1', 'UCB2.2'='0.5')


# Maximum number of observations
max_obs=2500

# data.frame aggregated for two versions: 20 and 40 arms
df_history_agg <- df_Yahoo_UCB %>%
  group_by(agent, sim)%>% # group by number of arms, the sim
  mutate(cumulative_reward = cumsum(reward))%>% # calculate cumulative sum
  group_by(agent, t) %>% # group by number of arms, the t
  summarise(avg_cumulative_reward = mean(cumulative_reward),# calc cumulative reward, se, CI
            se_cumulative_reward = sd(cumulative_reward, na.rm=TRUE)/sqrt(n_sim)) %>%
  mutate(cumulative_reward_lower_CI =avg_cumulative_reward - 1.96*se_cumulative_reward,
         cumulative_reward_upper_CI =avg_cumulative_reward + 1.96*se_cumulative_reward)%>%
  filter(t <=max_obs)


# create ggplot object
ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward, color =agent))+
  geom_line(size=1.5)+
  geom_ribbon(aes(ymin=cumulative_reward_lower_CI , 
                  ymax=cumulative_reward_upper_CI,
                  fill = agent,
  ),
  alpha=0.1)+
  labs(x = 'Time', y='Cumulative Reward', color ='c', fill='c')+
  theme_bw()+
  theme(text = element_text(size=16))
  
```


\textbf{Task 7: compare the UCB policy for $c=0.1$ to an $\epsilon$-greedy policy where $\epsilon=0.1$. Which one does better? Why do you think this might be?}. Repeat this exercise again for a maximum of $t=10.000$. Hint: use the _EpsilonGreedyPolicy$new(epsilon = 0.1)_ function from the _contextual_ package.


\textbf{Answer}: Using the $\epsilon$-greedy policy where $\epsilon=0.1$ scores on average better. This might be because exploring random arms is better than getting 'stuck' in arms that might seem to have potential. However, the difference between the two policies  is not statistically significant, as the 95\% confidence intervals overlap. 


```{r,results='hide', message=FALSE, warning=FALSE}

# set the seed
set.seed(0)

# define epsilon greedy policy
eps_greedy <- EpsilonGreedyPolicy$new(epsilon = 0.1)

# define agent
eps_greedy_agent <- Agent$new(eps_greedy, bandit_yahoo)


# simulate
simulator          <- Simulator$new(list(UCB_01_agent, eps_greedy_agent), # set our agents
                                      horizon= size_sim, # set the sizeof each simulation
                                      do_parallel = TRUE, # run in parallel for speed
                                      simulations = n_sim, # simulate it n_sim times
                                  )
  
  
# run the simulator object  
history_compare_UCB_eps_greedy            <- simulator$run()
```

```{r}

# select relevant columns
df_Yahoo_UCB_eps_greedy <- history_compare_UCB_eps_greedy$data  %>%
  select(t, sim, choice, reward, agent) 
  

# data.frame aggregated for two agents: UCB and epsilon greedy
df_history_agg <- df_Yahoo_UCB_eps_greedy %>%
  group_by(agent, sim)%>% # group by number of arms, the sim
  mutate(cumulative_reward = cumsum(reward))%>% # calculate cumulative sum
  group_by(agent, t) %>% # group by number of arms, the t
  summarise(avg_cumulative_reward = mean(cumulative_reward),# calc cumulative reward, se, CI
            se_cumulative_reward = sd(cumulative_reward, na.rm=TRUE)/sqrt(n_sim)) %>%
  mutate(cumulative_reward_lower_CI =avg_cumulative_reward - 1.96*se_cumulative_reward,
         cumulative_reward_upper_CI =avg_cumulative_reward + 1.96*se_cumulative_reward)%>%
  filter(t <=max_obs)



```

```{r}
# create ggplot object
ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward, color =agent))+
  geom_line(size=1.5)+
  geom_ribbon(aes(ymin=cumulative_reward_lower_CI , 
                  ymax=cumulative_reward_upper_CI,
                  fill = agent,
  ),
  alpha=0.1)+
  labs(x = 'Time', y='Cumulative Reward', color ='Policy', fill='Policy')+ # set titles
  theme_bw()+ # set theme
  theme(text = element_text(size=16))+ # enlarge text
  scale_color_manual(values = c("#D16103", "#4E84C4"))


```

\textbf{Task 8: to better understand the $\epsilon$-greedy policy and UCB algorithm, plot for each the \% of the time a certain arm is chosen. Which arm is chosen most often? does this differ between the two approaches?


```{r}

# dataframe with arm choices for UCB
df_arm_choices_UCB_01 <- df_Yahoo_UCB_01%>%
  complete(t, sim,choice,fill = list(n = 0)) %>%
  group_by(t, sim,choice, .drop=FALSE) %>%
  mutate(n = sum(!is.na(reward))) %>%
  group_by(t, choice, .drop=FALSE)%>%
  summarise(sum_n = sum(n)) %>%
  group_by(choice, .drop=FALSE)%>%
  mutate(cum_sum_n = cumsum(sum_n), 
         avg_cum_sum_n = cum_sum_n / (10 *t)) %>%
  filter(t <=max_obs)



ggplot(data = df_arm_choices_UCB_01, aes(x=t, y=avg_cum_sum_n, color = as.factor(choice)))+
  geom_line(size=1.5)+
  xlim(500, 2500)+
  theme_bw()+
  scale_color_brewer(palette="Spectral")+
  labs(color = 'Arm', x = 'Timestep', y='Average % of arms chosen ', main ='Arm Choices UCB')+
  theme(text = element_text(size=16))


```


```{r}

# dataframe with arm choices for Epsilon Greedy
df_arm_choices_EPS_01 <- df_Yahoo_UCB_eps_greedy%>%
  complete(t, sim,choice,fill = list(n = 0)) %>%
  group_by(t, sim,choice, .drop=FALSE) %>%
  mutate(n = sum(!is.na(reward))) %>%
  group_by(t, choice, .drop=FALSE)%>%
  summarise(sum_n = sum(n)) %>%
  group_by(choice, .drop=FALSE)%>%
  mutate(cum_sum_n = cumsum(sum_n), 
         avg_cum_sum_n = cum_sum_n / (10 *t)) %>%
  filter(t <=max_obs)

ggplot(data = df_arm_choices_EPS_01, aes(x=t, y=avg_cum_sum_n, color = as.factor(choice)))+
  geom_line(size=1.5)+
  xlim(500, 2500)+
  theme_bw()+
  scale_color_brewer(palette="Spectral")+
  labs(color = 'Arm', x = 'Timestep', y='Average % of arms chosen ', main ='Arm Choices Epsilon-Greedy')+
  theme(text = element_text(size=16))




```
